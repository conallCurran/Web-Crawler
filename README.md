# Web-Crawler

API's Used:
*.NET Framework 4.7, 4.6, and 4.5


What the project does:
    This program was design to list every url within a given website by creating Web requests, retrieving the response and printing them to a html file using streamReader and Writer outputs. This writes all of the urls to a response txt file of all urls on that page and allows the user to open txt file and read through urls from that page. This was my first attempt at developing a program designed to test all urls within a given website as part of QA tools for Web Developers and Producers. 


Why the project is useful:
        When I worlked as a Web  producer and developer I wanted to automate part of the QA process to overcome the tedious tasks of checking content for grammer and spelling errors and also checkign for broken links. I had hoped to develop a prgram that would possibly run through all of the content of a page and not only check for 404,403 errors but also check for typos in web content.



How users can get started with the project:
          This could be used as a starting point for anyone else looking to develop Web tools and can be integrated into a bigger overall QA software. For me this is only really a start to build something bigger with a better interface.


Where users can get help with your project:
        https://stackoverflow.com/questions/4015324/http-request-with-post
        https://stackoverflow.com/questions/37462135/how-make-an-http-request-in-c-sharp?noredirect=1&lq=1
        https://stackoverflow.com/questions/4015324/http-request-with-post?noredirect=1&lq=1
        http://www.c-sharpcorner.com/article/using-webrequest-and-webresponse-classes/


Who maintains and contributes to the project:
        Developer.
